# -*- coding: utf-8 -*-
"""AL_ml_intership.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ngomMKcxn_iJn1zmCwRvStToALfnYUIN
"""

# Commented out IPython magic to ensure Python compatibility.
# importing the libraries for data processing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
# %matplotlib inline
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
sns.set()
import re
import nltk
# color pallette
cnf = '#393e46' # confirmed - grey
dth = '#ff2e63' # death - red
rec = '#21bf73' # recovered - cyan
act = '#fe9801' # active case - yellow

# converter
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()   

# hide warnings
import warnings
warnings.filterwarnings('ignore')


# html embedding
from IPython.display import Javascript
from IPython.core.display import display
from IPython.core.display import HTML

train = pd.read_csv('Tweet_NFT.csv')
test = pd.read_csv('Tweet_NFT.csv')

train.info()

train.tail()

train.head()

#checking for null values in the train and test data

train.isnull().any()
test.isnull().any()

#check for identified intent: Appreciation, Community, Done,Giveaway, Interested, Launching Soon, PinkSale, PreSale, Whitelist

train[train['tweet_intent'] ==1].head(50)

#checking the negative tweet_intent in the train data

train[train['tweet_intent'] == 0].head(10)

# checking the distribution of tweet_intent counts in the train data

train['tweet_intent'].value_counts().plot.bar(color = 'brown', figsize = (10,5))

# checking the distribution of tweets in the whole dataset

length_train = train['tweet_text'].str.len().plot.hist(color = 'pink', figsize = (10, 5))
length_test = test['tweet_text'].str.len().plot.hist(color = 'green', figsize = (10,5))

# adding a column to check for the length of the tweet

train['len'] = train['tweet_text'].str.len()
test['len'] =  test['tweet_text'].str.len()

# checking the newly added column that checks the length of a tweet
train.head(10)

# count vectorization for the text
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words = 'english')
words = cv.fit_transform(train.tweet_text)

sum_words = words.sum(axis=0)

words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)

frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])

frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'brown')
plt.title("Most Frequently Occuring Words - Top 30")

# generating word cloud for the most common occuring words in the train data
from wordcloud import WordCloud

wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))

plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.title("WordCloud - Vocabulary from Reviews", fontsize = 22)

# wordcloud for words that are neutral
normal_words =' '.join([text for text in train['tweet_text']])

wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(normal_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title('The Neutral Words')
plt.show()

# defining a function to collect the hashtags from the train data
def hashtags_extract(x):
    hashtags = []
    for i in x:
        ht = re.findall(r"#(\w+)", i)
        hashtags.append(ht)
    
    return hashtags

#extracting hashtags from racist/sexist tweet
ht_normal = hashtags_extract(train['tweet_text'])

#extracting hashtags from normal tweet
ht_negetive = hashtags_extract(train['tweet_text'])

#unnesting list

ht_normal = sum(ht_normal , [])
ht_negetive = sum(ht_negetive , [])

# Visualizing the most common hashtags in the train data
a = nltk.FreqDist(ht_normal)

d = pd.DataFrame({'Hashtag': list(a.keys()),
                  'Count': list(a.values())})

# selecting top 20 most frequent hashtags     
d = d.nlargest(columns="Count", n = 20) 
plt.figure(figsize=(15,5))
ax = sns.barplot(data=d, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

a = nltk.FreqDist(ht_negetive)
d = pd.DataFrame({'Hashtag': list(a.keys()),
                  'Count': list(a.values())})

# selecting top 20 most frequent hashtags     
d = d.nlargest(columns="Count", n = 20) 
plt.figure(figsize=(16,5))
ax = sns.barplot(data=d, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

# tokenizing the words present in the training set
tokenized_tweet = train['tweet_text'].apply(lambda x: x.split()) 

# importing gensim
import gensim

# creating a word to vector model
model_w2v = gensim.models.Word2Vec(
            tokenized_tweet,
            size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=2,
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)

model_w2v.train(tokenized_tweet, total_examples= len(train['tweet_text']), epochs=20)

model_w2v.wv.most_similar(positive = "nft")

model_w2v.wv.most_similar(positive = "BSC")

model_w2v.wv.most_similar(positive = "Giveaway")

model_w2v.wv.most_similar(negative = "Presale")

from tqdm import tqdm
tqdm.pandas(desc="progress-bar")
from gensim.models.doc2vec import LabeledSentence

def add_label(twt):
    output = []
    for i, s in zip(twt.index, twt):
        output.append(LabeledSentence(s, ["tweet_text_" + str(i)]))
    return output

# label all the tweets
labeled_tweets = add_label(tokenized_tweet)

labeled_tweets[:6]

# removing unwanted patterns from the data

import re
import nltk

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

train_corpus = []

for i in range(0, 7920):
  review = re.sub('[^a-zA-Z]', ' ', train['tweet_text'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
   # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  train_corpus.append(review)

# creating bag of words for train
# creating bag of words

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features = 1000)
x = cv.fit_transform(train_corpus).toarray()
y = train.iloc[:, 1]

print(x.shape)
print(y.shape)

test_corpus = []

for i in range(0, 1953):
  review = re.sub('[^a-zA-Z]', ' ', test['tweet_text'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  test_corpus.append(review)

# creating bag of words for test

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features = 1500)
x_test = cv.fit_transform(test_corpus).toarray()
y = train.iloc[:, 1]

print(x_test.shape)